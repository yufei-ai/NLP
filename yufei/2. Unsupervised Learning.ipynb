{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing\n",
    "## 2. Unsupervised Learning\n",
    "### ASI Data Science Fellowship IX - 6th October 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Get Vader data for sentiment analysis\n",
    "import nltk\n",
    "nltk.download('vader_lexicon') \n",
    "\n",
    "# dictionary of colours for making nice plots later\n",
    "PARTY_COLOURS = {'trump': '#E91D0E', 'obama': '#00A6EF'}\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_tweet(text):\n",
    "    # encode tweets as utf-8 strings\n",
    "    text = text.decode('utf-8')\n",
    "    # remove commas in numbers (else vectorizer will split on them)\n",
    "    text = re.sub(r',([0-9])', '\\\\1', text)\n",
    "    # sort out HMTL formatting of &\n",
    "    text = re.sub(r'&amp', 'and', text)\n",
    "    # strip urls\n",
    "    return re.sub(r'http[s]{0,1}://[^\\s]*', '', text)\n",
    "\n",
    "df = pd.read_pickle('tweets.pkl')\n",
    "df['text'] = df['text'].map(clean_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we allow our vectorizer to infer a vocabulary from the corpus, then this will typically result in a huge number of sparesely populated features. We can often dimension reduce and retain relevant information (albeit sacrificing some interpretability), and improve the efficiency of our models and analysis.\n",
    "\n",
    "Let's visualise our tfidf vectors in a few different ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis\n",
    "\n",
    "Principal component analysis aims find a coordinate system where correlation between features is minimized. By keeping only the coordinate directions in the new system that explain the most variance, we can reduce the dimensions of our feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "def scatter(x, colors):\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    ax.scatter(x[(colors==0),0], x[(colors==0),1], c=PARTY_COLOURS['trump'], label='Trump', alpha=0.5)\n",
    "    ax.scatter(x[(colors==1),0], x[(colors==1),1], c=PARTY_COLOURS['obama'], label='Obama', alpha=0.5)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    plt.legend()\n",
    "    \n",
    "    return f, ax\n",
    "\n",
    "# dimension reduction algorithms can be pretty slow, so let's work with a sample\n",
    "# try on the whole data set if you want!\n",
    "sample_trump = df.loc[df['label'] == 0, ['text', 'label']].sample(500)\n",
    "sample_obama = df.loc[df['label'] == 1, ['text', 'label']].sample(500)\n",
    "sample = sample_trump.append(sample_obama)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=10000)\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(sample['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use `PCA` to reduce `tfidf_vectors` to two dimensions, then plot the results using the `scatter` function. Pass the labels as colours.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dimension reduce and plot here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE\n",
    "\n",
    "t-SNE is another dimension reduction algorithm, but one that is generally better at preserving the global structure of the data. In the case of our twitter data it does a much better job than PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# we have suggested some parameters below, feel free to experiment\n",
    "tsne = TSNE(perplexity = 800, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform `tfidf_vectors` using `tsne` and plot them using `scatter`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension reduce and plot here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a numeric representation of our data, there are many clustering algorithms we can try out. Since our feature vectors are extremely high dimensional, it is a good idea to first dimension reduce so that we do not fall foul of the curse of dimensionality.\n",
    "\n",
    "**Use KMeans to cluster your t-SNE vectors into two classes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# cluster tsne vectors using K-Means\n",
    "\n",
    "sample['kmeans_labels'] = # your kmeans labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some reorganizing for plotting clusters\n",
    "\n",
    "df_trump = sample[sample['label'] == 0]\n",
    "df_obama = sample[sample['label'] == 1]\n",
    "\n",
    "trump_counts = df_trump[['kmeans_labels', 'label']].groupby('kmeans_labels').count().values.flatten()\n",
    "obama_counts = df_obama[['kmeans_labels', 'label']].groupby('kmeans_labels').count().values.flatten()\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "bars11 = ax.bar(np.arange(2)-0.15, trump_counts, 0.3, color=PARTY_COLOURS['trump'], label='Trump')\n",
    "bars12 = ax.bar(np.arange(2)+0.15, obama_counts, 0.3, color=PARTY_COLOURS['obama'], label='Obama')\n",
    "plt.legend(fontsize=12)\n",
    "plt.ylabel('Count', fontsize=15)\n",
    "plt.xticks([0, 1])\n",
    "plt.xlabel('Cluster', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other clustering\n",
    "\n",
    "Let's also try some other clustering algorithms.\n",
    "\n",
    "**Try using spectral clustering to cluster the t-SNE vectors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# spectral cluster the t-SNE vectors here\n",
    "\n",
    "sample['scl_labels'] = # your spectral cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some reorganizing for plotting clusters\n",
    "\n",
    "df_trump = sample[sample['label'] == 0]\n",
    "df_obama = sample[sample['label'] == 1]\n",
    "\n",
    "trump_counts = df_trump[['scl_labels', 'label']].groupby('scl_labels').count().values.flatten()\n",
    "obama_counts = df_obama[['scl_labels', 'label']].groupby('scl_labels').count().values.flatten()\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "bars11 = ax.bar(np.arange(2)-0.15, trump_counts, 0.3, color=PARTY_COLOURS['trump'], label='Trump')\n",
    "bars12 = ax.bar(np.arange(2)+0.15, obama_counts, 0.3, color=PARTY_COLOURS['obama'], label='Obama')\n",
    "plt.legend(fontsize=12)\n",
    "plt.ylabel('Count', fontsize=15)\n",
    "plt.xticks([0, 1])\n",
    "plt.xlabel('Cluster', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print a selection of Obama tweets that ended up in the Trump cluster, and a selection of Trump tweets that ended up in the Obama cluster. If the clustering is working well, the Obama tweets should look Trumpian, and the Trump tweets should look Obama-like.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print tweets that ended up in the wrong clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) is a statistical model of topic distribution that can be trained using bayesian inference and Markov Chain Monte Carlo methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_vectorizer = TfidfVectorizer(\n",
    "    stop_words='english', max_df=0.95, min_df=2, use_idf=False, max_features=5000\n",
    ")\n",
    "tf_vectors = tf_vectorizer.fit_transform(df['text'])\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=5, max_iter=20, random_state=42, learning_method='batch'\n",
    ")\n",
    "\n",
    "lda_vectors = lda.fit_transform(tf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words = \" \".join(\n",
    "            [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        )\n",
    "        print(\"Topic #{}: {}\".format(topic_idx, top_words))\n",
    "    print()\n",
    "\n",
    "print_top_words(lda, tf_vectorizer.get_feature_names(), 10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obama_tf = tf_vectorizer.transform(\n",
    "    df[df['label'] == 1]['text']\n",
    ")\n",
    "obama_lda = lda.transform(obama_tf)\n",
    "\n",
    "trump_tf = tf_vectorizer.transform(\n",
    "    df[df['label'] == 0]['text']\n",
    ")\n",
    "trump_lda = lda.transform(trump_tf)\n",
    "\n",
    "obama_topics = np.mean(obama_lda, axis=0)\n",
    "trump_topics = np.mean(trump_lda, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "topic_words = [[feature_names[i] for i in topic.argsort()[:-6:-1]] for topic in lda.components_]\n",
    "\n",
    "\n",
    "def major_formatter(x, pos):\n",
    "    return str('\\n'.join(topic_words[int(x)][0:5]))\n",
    "\n",
    "bars1 = ax.bar(np.arange(5)-0.15, obama_topics, 0.3, color=PARTY_COLOURS['obama'], label='Obama')\n",
    "bars2 = ax.bar(np.arange(5)+0.15, trump_topics, 0.3, color=PARTY_COLOURS['trump'], label='Trump')\n",
    "plt.legend(fontsize=12)\n",
    "plt.ylabel('Relevance', fontsize=15)\n",
    "plt.xticks([0, 1, 2, 3, 4])\n",
    "plt.gca().xaxis.set_major_formatter(ticker.FuncFormatter(major_formatter))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some stuff about sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "df_sentiment = (\n",
    "    df[df['label'] == 0].sample(200).append(\n",
    "        df[df['label'] == 1].sample(200)\n",
    "    ).copy()\n",
    ")\n",
    "\n",
    "for i, tweet in df_sentiment['text'].iteritems():\n",
    "    ss = sid.polarity_scores(str(tweet))\n",
    "    for k in sorted(ss):\n",
    "        df_sentiment.loc[i, k] = ss[k]\n",
    "\n",
    "df_sentiment\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a plot here comparing sentiment distribution of Trump vs. Obama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_sentiment = df_sentiment[df_sentiment['label'] == 0][['compound', 'neg', 'pos']]\n",
    "obama_sentiment = df_sentiment[df_sentiment['label'] == 1][['compound', 'neg', 'pos']]\n",
    "\n",
    "trump_neg_sentiment = trump_sentiment['neg'].sort_values().reset_index(drop=True)\n",
    "obama_neg_sentiment = obama_sentiment['neg'].sort_values().reset_index(drop=True)\n",
    "\n",
    "trump_pos_sentiment = trump_sentiment['pos'].sort_values().reset_index(drop=True)\n",
    "obama_pos_sentiment = obama_sentiment['pos'].sort_values().reset_index(drop=True)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2)\n",
    "ax[0].plot(trump_neg_sentiment, c=PARTY_COLOURS['trump'], label='Trump')\n",
    "ax[0].plot(obama_neg_sentiment, c=PARTY_COLOURS['obama'], label='Obama')\n",
    "ax[1].plot(trump_pos_sentiment, c=PARTY_COLOURS['trump'], label='Trump')\n",
    "ax[1].plot(obama_pos_sentiment, c=PARTY_COLOURS['obama'], label='Obama')\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "ax[0].set_ylabel('Negative sentiment', fontsize=10)\n",
    "ax[1].set_ylabel('Positive sentiment', fontsize=10)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
