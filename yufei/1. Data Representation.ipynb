{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing\n",
    "## 1. Data Representation\n",
    "### ASI Data Science Fellowship IX - 6th October 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Before you do anything, apply the 'requirements' environment to your server to make sure that you have all the modules we are going to need for the examples below.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# dictionary of colours for making nice plots later\n",
    "PARTY_COLOURS = {'trump': '#E91D0E', 'obama': '#00A6EF'}\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will take you through many of the concepts we have introduced in this session. We will use the same dataset for all examples, namely a collection of 6000 or so tweets from @realDonaldTrump and @BarackObama. \n",
    "\n",
    "Wherever possible we will use `sklearn`, python's machine learning library that you are most likely already familiar with. For a few tasks we will turn to `nltk` (natural language toolkit) a python library for NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-02-08 15:23:29</td>\n",
       "      <td>829349943613734912</td>\n",
       "      <td>b'Thank you to our great Police Chiefs &amp;amp; S...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-09-19 16:26:07</td>\n",
       "      <td>513001099302023168</td>\n",
       "      <td>b'\"This is not your fight alone. It\\'s on all ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-05-23 15:43:16</td>\n",
       "      <td>867043258932875264</td>\n",
       "      <td>b'Thank you for such a wonderful and unforgett...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-06-15 10:55:37</td>\n",
       "      <td>875305788708974592</td>\n",
       "      <td>b'They made up a phony collusion with the Russ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-06-23 20:07:16</td>\n",
       "      <td>613438190829965312</td>\n",
       "      <td>b\"It doesn't get much better than the chance f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date                  id  \\\n",
       "0 2017-02-08 15:23:29  829349943613734912   \n",
       "1 2014-09-19 16:26:07  513001099302023168   \n",
       "2 2017-05-23 15:43:16  867043258932875264   \n",
       "3 2017-06-15 10:55:37  875305788708974592   \n",
       "4 2015-06-23 20:07:16  613438190829965312   \n",
       "\n",
       "                                                text  label  \n",
       "0  b'Thank you to our great Police Chiefs &amp; S...      0  \n",
       "1  b'\"This is not your fight alone. It\\'s on all ...      1  \n",
       "2  b'Thank you for such a wonderful and unforgett...      0  \n",
       "3  b'They made up a phony collusion with the Russ...      0  \n",
       "4  b\"It doesn't get much better than the chance f...      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('tweets.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many things to consider when cleaning text data. Some problems are common to other data types, such as how to deal with missing values. Others are unique to text data, and include things like removing HTML tags or urls. We don't want to focus too much on data cleaning for the purposes of this course, we've done a little bit of cleaning below to give you a taste. Generally speaking regular expressions (available in python in the `re` module) will get you pretty far. For specific tasks there are often existing libraries you can use. For example `feedparser` is good for getting data from an RSS feed, `beautifulsoup` is good for parsing HTML/XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_tweet(text):\n",
    "    # encode tweets as utf-8 strings\n",
    "    text = text.decode('utf-8')\n",
    "    # remove commas in numbers (else vectorizer will split on them)\n",
    "    text = re.sub(r',([0-9])', '\\\\1', text)\n",
    "    # sort out HMTL formatting of &\n",
    "    text = re.sub(r'&amp', 'and', text)\n",
    "    # strip urls\n",
    "    return re.sub(r'http[s]{0,1}://[^\\s]*', '', text)\n",
    "\n",
    "df['text'] = df['text'].map(clean_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The field of natural language processing contains a lot of jargon from linguistics. We don't want to get too bogged down in defining lots of new terms, but the following two are helpful:\n",
    "\n",
    "- Type: An element of the vocabulary. May be a word, may be an n-gram (ordered sequence of words)\n",
    "- Token: An instance of a type in running text.\n",
    "\n",
    "Any given language has a large enough vocabulary that trying to do data science on the set of all possible sentences is totally impractical. Instead it helps to break text up into smaller chunks, a process called tokenizing.\n",
    "\n",
    "Exactly how we do this will depend on the problem, but some common ways include splitting on whitespace, or splitting on non-alphanumeric characters. In general, the method of tokenizing will be informed by the format of the text data being studied.\n",
    "\n",
    "**Tokenizers are accessed in a slightly roundabout way in `sklearn` as below. Run this cell a few times to tokenize random tweets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['For',\n",
       " 'all',\n",
       " 'that',\n",
       " 'Beau',\n",
       " 'Biden',\n",
       " 'achieved',\n",
       " 'in',\n",
       " 'his',\n",
       " 'life',\n",
       " 'nothing',\n",
       " 'claimed',\n",
       " 'fuller',\n",
       " 'focus',\n",
       " 'of',\n",
       " 'his',\n",
       " 'love',\n",
       " 'and',\n",
       " 'devotion',\n",
       " 'than',\n",
       " 'his',\n",
       " 'family',\n",
       " 'President',\n",
       " 'Obama']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from random import randint\n",
    "\n",
    "# tokenize a random tweet\n",
    "i = randint(0, len(df) - 1)\n",
    "tokenizer = CountVectorizer().build_tokenizer()\n",
    "tokenizer(df['text'].iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing breaks our raw text data down into more manageable chunks, but it's still not in a form that is particularly useful for training models. Let's look at a few common, simple ways of vectorizing text data. We will use `sklearn` which can efficiently vectorize text data and stores everything as `scipy` sparse arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the simplest way to vectorize is to simply create a vector of counts of the number of times any type appears in a given piece of text.\n",
    "\n",
    "To get some intuition, let's try it on a small test corpus of 10 random tweets.\n",
    "\n",
    "**Use `sample` on the series `df['text']` to get a random selection of 10 tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5768, 5213, 1650, 2375, 3265, 5001, 3116, 3258, 5173, 2622]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.sample(range(len(df)),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "test_corpus = df['text'].iloc[random.sample(range(len(df)),10)]\n",
    "\n",
    "test_corpus = test_corpus.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the sample, we can create count vectors using `CountVectorizer` from `sklearn`. We set `max_features=5` so as to work with a small vocabulary of only the most common terms.\n",
    "\n",
    "See the next cell for usage of `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['for', 'rt', 'the', 'to', 'we']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a count vectorizer with our desired parameters\n",
    "count_vectorizer = CountVectorizer(max_features=5)\n",
    "\n",
    "# first 'fit' the vectorizer to the corpus\n",
    "# this step automatically determines the vocabulary\n",
    "count_vectorizer.fit(test_corpus)\n",
    "\n",
    "# then 'transform' the corpus to count vectors (a matrix)\n",
    "count_vectors = count_vectorizer.transform(test_corpus)\n",
    "\n",
    "count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will display each of your tweets and the corresponding counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you to former campaign adviser Michael Caputo for saying so powerfully that there was no Russian collusion in our winning campaign.\n",
      "Counts Terms\n",
      "     0   for\n",
      "     0    rt\n",
      "     0   the\n",
      "     1    to\n",
      "     1    we\n",
      "----------------------------------------\n",
      "Let's keep fighting for real, lasting change: \n",
      "Counts Terms\n",
      "     0   for\n",
      "     0    rt\n",
      "     0   the\n",
      "     0    to\n",
      "     0    we\n",
      "----------------------------------------\n",
      "Great bilateral meetings at Élysée Palace w/ President @EmmanuelMacron. The friendship between our two nations and… \n",
      "Counts Terms\n",
      "     0   for\n",
      "     0    rt\n",
      "     3   the\n",
      "     1    to\n",
      "     0    we\n",
      "----------------------------------------\n",
      "Working in Bedminster, N.J., as long planned construction is being done at the White House. This is not a vacation - meetings and calls!\n",
      "Counts Terms\n",
      "     0   for\n",
      "     1    rt\n",
      "     2   the\n",
      "     0    to\n",
      "     1    we\n",
      "----------------------------------------\n",
      "More Anti-Catholic Emails From Team Clinton:  \n",
      "Counts Terms\n",
      "     1   for\n",
      "     0    rt\n",
      "     1   the\n",
      "     2    to\n",
      "     1    we\n",
      "----------------------------------------\n",
      "Melania and I offer our deepest condolences to the family of Otto Warmbier. Full statement:  \n",
      "Counts Terms\n",
      "     0   for\n",
      "     0    rt\n",
      "     0   the\n",
      "     0    to\n",
      "     0    we\n",
      "----------------------------------------\n",
      "After nine years, the best source of \"truthiness\" is coming to an end. Congratulations @StephenAtHome. \n",
      "Counts Terms\n",
      "     1   for\n",
      "     2    rt\n",
      "     1   the\n",
      "     1    to\n",
      "     1    we\n",
      "----------------------------------------\n",
      "RT @WhiteHouse: Here's how today's new actions will spur innovation and promote more efficient vehicles:  \n",
      "Counts Terms\n",
      "     1   for\n",
      "     0    rt\n",
      "     1   the\n",
      "     0    to\n",
      "     0    we\n",
      "----------------------------------------\n",
      "You don't want to miss this. Say you'll tune in for President Obama's final #SOTU:  \n",
      "Counts Terms\n",
      "     1   for\n",
      "     0    rt\n",
      "     2   the\n",
      "     0    to\n",
      "     0    we\n",
      "----------------------------------------\n",
      "LIVE: President Obama is delivering remarks at the #WHScienceFair. \n",
      "Counts Terms\n",
      "     0   for\n",
      "     0    rt\n",
      "     0   the\n",
      "     2    to\n",
      "     0    we\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "features = count_vectorizer.get_feature_names()\n",
    "\n",
    "# we use .toarray() to convert from sparse \n",
    "# array to dense numpy array\n",
    "for i, row in enumerate(count_vectors.toarray()):\n",
    "    print(test_corpus[i])\n",
    "    print(pd.DataFrame({'Terms': features, 'Counts': row}).to_string(index=False))\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term frequency vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count vectors are very sensitive to document length. In our case we expect all tweets to be similar lengths, but in general we might be dealing with documents of varying lengths, so it makes sense to normalise the count vectors. This results in so-called frequency vectors.\n",
    "\n",
    "** Using `TfidfVectorizer`, compute term frequency vectors for the test corpus and print them out as we did for the count vectors. Make sure you set `use_idf=False` when initialising your `TfidfVectorizer`. As before limit the vocabulary to 5 types.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'in', 'is', 'the', 'to']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_vectorizer = TfidfVectorizer(max_features=5, use_idf = False)\n",
    "tf_corpus = tf_vectorizer.fit_transform(test_corpus)\n",
    "tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you to former campaign adviser Michael Caputo for saying so powerfully that there was no Russian collusion in our winning campaign.\n",
      "Counts Terms\n",
      "0.000000   and\n",
      "0.707107    in\n",
      "0.000000    is\n",
      "0.000000   the\n",
      "0.707107    to\n",
      "----------------------------------------\n",
      "Let's keep fighting for real, lasting change: \n",
      "Counts Terms\n",
      "   0.0   and\n",
      "   0.0    in\n",
      "   0.0    is\n",
      "   0.0   the\n",
      "   0.0    to\n",
      "----------------------------------------\n",
      "Great bilateral meetings at Élysée Palace w/ President @EmmanuelMacron. The friendship between our two nations and… \n",
      "Counts Terms\n",
      "0.707107   and\n",
      "0.000000    in\n",
      "0.000000    is\n",
      "0.707107   the\n",
      "0.000000    to\n",
      "----------------------------------------\n",
      "Working in Bedminster, N.J., as long planned construction is being done at the White House. This is not a vacation - meetings and calls!\n",
      "Counts Terms\n",
      "0.377964   and\n",
      "0.377964    in\n",
      "0.755929    is\n",
      "0.377964   the\n",
      "0.000000    to\n",
      "----------------------------------------\n",
      "More Anti-Catholic Emails From Team Clinton:  \n",
      "Counts Terms\n",
      "   0.0   and\n",
      "   0.0    in\n",
      "   0.0    is\n",
      "   0.0   the\n",
      "   0.0    to\n",
      "----------------------------------------\n",
      "Melania and I offer our deepest condolences to the family of Otto Warmbier. Full statement:  \n",
      "Counts Terms\n",
      "0.57735   and\n",
      "0.00000    in\n",
      "0.00000    is\n",
      "0.57735   the\n",
      "0.57735    to\n",
      "----------------------------------------\n",
      "After nine years, the best source of \"truthiness\" is coming to an end. Congratulations @StephenAtHome. \n",
      "Counts Terms\n",
      "0.00000   and\n",
      "0.00000    in\n",
      "0.57735    is\n",
      "0.57735   the\n",
      "0.57735    to\n",
      "----------------------------------------\n",
      "RT @WhiteHouse: Here's how today's new actions will spur innovation and promote more efficient vehicles:  \n",
      "Counts Terms\n",
      "   1.0   and\n",
      "   0.0    in\n",
      "   0.0    is\n",
      "   0.0   the\n",
      "   0.0    to\n",
      "----------------------------------------\n",
      "You don't want to miss this. Say you'll tune in for President Obama's final #SOTU:  \n",
      "Counts Terms\n",
      "0.000000   and\n",
      "0.707107    in\n",
      "0.000000    is\n",
      "0.000000   the\n",
      "0.707107    to\n",
      "----------------------------------------\n",
      "LIVE: President Obama is delivering remarks at the #WHScienceFair. \n",
      "Counts Terms\n",
      "0.000000   and\n",
      "0.000000    in\n",
      "0.707107    is\n",
      "0.707107   the\n",
      "0.000000    to\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "features = tf_vectorizer.get_feature_names()\n",
    "for i, row in enumerate(tf_corpus.toarray()):\n",
    "    print(test_corpus[i])\n",
    "    print(pd.DataFrame({'Terms': features, 'Counts': row}).to_string(index=False))\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfidf vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf-idf stands for 'term frequency - inverse document frequency'. Given our term frequencys, we re-weight by the inverse of the document frequency. Therefore a given term will have a larger value if it both appears many times in the document, but appears infrequently across the corpus. In this sense it automatically detects and upweights terms which are likely to be able to help us distinguish between documents.\n",
    "\n",
    "**Compute tfidf vectors for your test_corpus. You can once again use `TfidfVectorizer`, but this time set `use_idf=True`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'in', 'is', 'the', 'to']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=5, use_idf=True)# your code here\n",
    "tf_corpus = tf_vectorizer.fit_transform(test_corpus)\n",
    "tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you to former campaign adviser Michael Caputo for saying so powerfully that there was no Russian collusion in our winning campaign.\n",
      "Counts Terms\n",
      "0.000000   and\n",
      "0.707107    in\n",
      "0.000000    is\n",
      "0.000000   the\n",
      "0.707107    to\n",
      "----------------------------------------\n",
      "Let's keep fighting for real, lasting change: \n",
      "Counts Terms\n",
      "   0.0   and\n",
      "   0.0    in\n",
      "   0.0    is\n",
      "   0.0   the\n",
      "   0.0    to\n",
      "----------------------------------------\n",
      "Great bilateral meetings at Élysée Palace w/ President @EmmanuelMacron. The friendship between our two nations and… \n",
      "Counts Terms\n",
      "0.707107   and\n",
      "0.000000    in\n",
      "0.000000    is\n",
      "0.707107   the\n",
      "0.000000    to\n",
      "----------------------------------------\n",
      "Working in Bedminster, N.J., as long planned construction is being done at the White House. This is not a vacation - meetings and calls!\n",
      "Counts Terms\n",
      "0.377964   and\n",
      "0.377964    in\n",
      "0.755929    is\n",
      "0.377964   the\n",
      "0.000000    to\n",
      "----------------------------------------\n",
      "More Anti-Catholic Emails From Team Clinton:  \n",
      "Counts Terms\n",
      "   0.0   and\n",
      "   0.0    in\n",
      "   0.0    is\n",
      "   0.0   the\n",
      "   0.0    to\n",
      "----------------------------------------\n",
      "Melania and I offer our deepest condolences to the family of Otto Warmbier. Full statement:  \n",
      "Counts Terms\n",
      "0.57735   and\n",
      "0.00000    in\n",
      "0.00000    is\n",
      "0.57735   the\n",
      "0.57735    to\n",
      "----------------------------------------\n",
      "After nine years, the best source of \"truthiness\" is coming to an end. Congratulations @StephenAtHome. \n",
      "Counts Terms\n",
      "0.00000   and\n",
      "0.00000    in\n",
      "0.57735    is\n",
      "0.57735   the\n",
      "0.57735    to\n",
      "----------------------------------------\n",
      "RT @WhiteHouse: Here's how today's new actions will spur innovation and promote more efficient vehicles:  \n",
      "Counts Terms\n",
      "   1.0   and\n",
      "   0.0    in\n",
      "   0.0    is\n",
      "   0.0   the\n",
      "   0.0    to\n",
      "----------------------------------------\n",
      "You don't want to miss this. Say you'll tune in for President Obama's final #SOTU:  \n",
      "Counts Terms\n",
      "0.000000   and\n",
      "0.707107    in\n",
      "0.000000    is\n",
      "0.000000   the\n",
      "0.707107    to\n",
      "----------------------------------------\n",
      "LIVE: President Obama is delivering remarks at the #WHScienceFair. \n",
      "Counts Terms\n",
      "0.000000   and\n",
      "0.000000    in\n",
      "0.707107    is\n",
      "0.707107   the\n",
      "0.000000    to\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# print out your vectors in a nice way\n",
    "features = tf_vectorizer.get_feature_names()\n",
    "for i, row in enumerate(tf_corpus.toarray()):\n",
    "    print(test_corpus[i])\n",
    "    print(pd.DataFrame({'Terms': features, 'Counts': row}).to_string(index=False))\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "\n",
    "So far we have only considered individual words and their frequencies. We lose a lot of information doing so, because we discard word order and grammar etc.\n",
    "\n",
    "A simple solution to this is to use n-grams, that is sequences of words of length n, when we tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['actions will', 'at the', 'president obama', 'so powerfully', 'source of']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can tokenize/vectorize with n-grams using the parameter\n",
    "# ngram_range. It takes a tuple of ints that specify min and max\n",
    "# n-gram lengths\n",
    "ngram_vectorizer = CountVectorizer(max_features=5, ngram_range=(2,2))\n",
    "\n",
    "ngram_vectorizer.fit_transform(test_corpus)\n",
    "ngram_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use `ngram_vectorizer` to compute bigram count vectors for your test corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print your vectors in a nice way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same priciples as before, namely vectorizing using term frequencies or term frequency-inverse document frequencies, apply here too.\n",
    "\n",
    "A big advantage of tokenizing using n-grams is that models can learn some basic information about which words tend to appear together, and which words follow on from other sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Model\n",
    "\n",
    "We've written a tweet generator that uses n-grams and a simple Markov model to generate new tweets based on some training data. You can see that when using unigrams, it just returns a random collection of words that follow the same distribution as the observed data. However bigrams and trigrams already manage to capture a lot of information about how words are used together.\n",
    "\n",
    "You can try the model using 10-grams or some large value of n too. But at that point there is not enough training data to make the Markov model particularly interesting. The model will just start to repeat actual tweets rather than generating new content. It will have massively overfit the data.\n",
    "\n",
    "If you have time, feel free to dive into the source code to see how the generator works, but you are also more than welcome to just use it as a black box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained!\n",
      "Model trained!\n",
      "Model trained!\n"
     ]
    }
   ],
   "source": [
    "from asi_nlp.twitter import TweetGenerator\n",
    "\n",
    "unigram_generator = TweetGenerator(1)\n",
    "unigram_generator.train(df[df['label'] == 0]['text'])\n",
    "\n",
    "bigram_generator = TweetGenerator(2)\n",
    "bigram_generator.train(df[df['label'] == 0]['text'])\n",
    "\n",
    "trigram_generator = TweetGenerator(3)\n",
    "trigram_generator.train(df[df['label'] == 0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['covered their been news killer ms wikileaks time its with hillary korea in better together wrong of and voter enjoy']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_generator.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rt if we are fake news to meet ag to nato commander just arrived in big failure countries charge them and staff thought her to vote maga gopchairwoman']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_generator.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['settled the trump dossier is complete and total fabrication utter nonsense very unfair']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_generator.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
